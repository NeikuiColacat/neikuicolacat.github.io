##  VGGT 多个尺度的 几何信息 能够相互推断 

## 使用 AA ， 不用 CA , 

1. 每一个帧token激活值仅受该帧内容影响 ， 每帧的 token 分布更稳定，便于归一化
2. 如果多个图像的 token 混在一起，归一化会受到跨图像分布差异影响
3. 不使用CA ： VGGT 需要的是所有图像平等参与建模 ， 如果一个图A 的 Q 和 图 B 的 K 做点积 ， 不对称的建模会让 图像信息融合
4. CA需要 单独的 QKV 映射矩阵 
5. 如果是 CA 的话 ， 交替进行AA ， 下一层 做 单帧 att ， 一个 token 就会融合多个图像信息 ， 做不到单帧内 att
6. 使用 frame wise att 能让模型 知道 哪些 token 来自 哪一个帧数 ， 就不需要 位置编码 ， 而且推理的时候不会遇到大模型外推性的问题

## token 处理 

对一个输入图像 ， 生成一个 图像token ti

搞一个learnable token tg 相机token

注册 token tr 辅助对齐 跨帧建模

第一个帧 相机token 注册token 由于要建模 坐标 使用专门的一组 token

用 ti tg tr 按顺序 拼接起来

注册token 在 帧内 可以 与图像 token、相机 token一起参与交互：提升了几何建模的表达力
在 帧之间 可以 做一个 位置编码 

输出 
1. 图像 token 用于 预测 深度图、点图、轨迹
2. 相机token 预测相机参数 
3. 注册token ， 辅助建模 用的 ， 被丢弃了


如 ViT）使用位置编码，是为了告诉模型每个 patch 在图像中的二维位置 , 但在 VGGT 的任务中相同位置的 patch在不同视角下可能对应完全不同的三维点


## 相机 参数 输出 

作者在主干网络后，又专门加了 4 层自注意力（Self-Attention）只在这些相机 token 之间交互。

在这 4 层 attention 之后，每个相机 token 会接一个 全连接层（linear layer）映射为 相机参数

## DPT 

它的作用是恢复空间分辨率，让特征token 能对应回图像的每个像素位置。

生成 二维 特征图 Fi

Fi 通过 3x3 卷积 映射 到 深度图 和 点云图 

DPT 还会 生成 Ti 给 Tracking Head使用

模型还会在训练阶段 做 深度 和 点云 的  不确定性估计

## 4.3 节 有意思的 实验

1. 直接用 point head map 精度差
2. 先预测 depth map 和 相机参数 ， 然后再 反投影 生成 point head map 效果更好

一定程度上说明了 几何约束 的 效果 是 更有用的

## 消融实验 

去掉 相机 loss ， track loss ， depth loss  ， 误差 显著 升高 ， 尤其是 前两个 

## 实现细节 

Frame-wise SA: 仅在每帧内部做 self-attention，形状通常整理为 (B·F, HW, C)，一次并行处理所有帧。

Global SA: 跨帧全局 attention，形状整理为 (B, F·HW, C)，捕获时空一致性。

---

Flash attention: 需满足 dtype（fp16/bf16）、dropout=0、张量连续/合适的内存格式；优先走 scaled_dot_product_attention 的快路径。

---

QKNorm + LayerScale:

QKNorm 放在 Q/K 线性层之后、注意力打分之前，稳定 logits 方差。

LayerScale 仅作用于每个残差分支的输出；gamma 初始化 0.01，放错位置会影响训练稳定性。

---

DPT 多尺度 tap：

从第 4、11、17、23 个 block 取出特征进入 DPT 上采样头，多任务或多尺度监督更稳定。

DPT 头对每帧独立预测，避免对显存的二次放大。

---

DIno V2 encoder , DPT decoder , CoTracker2 用来做跟踪头


复习 Cross Attention

推理阶段不对模型输出做归一化，以保留网络学习到的真实尺度和数值分布

没有相机头

复习表格