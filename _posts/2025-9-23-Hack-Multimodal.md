# 多模态模型Hack研究LOG

## Day 1恶补一下常见主流多模态模型

初步idea是挑选一个输入RGB信息和Depth信息的模型然后在上面做实验

需要了解主流多模态模型是怎么做信息融合的

先挑选一些参数量小的模型，便于快速验证学习

由于对多模态模型了解比较少，所以类似与CLIP等比较经典的东西也可以看一看，学学怎么设计多模态的损失函数

### CLIP 模型 

把图像vec和单词vec投射到一个空间上，让符合的图像和单词pair在空间中尽可能匹配

用余弦相似度计算损失函数 ， 忽略向量模长，只考虑方向（即向量的语义信息）

按列做softmax + 按行做一次softmax 然后做交叉熵算损失函数 ，这样模型能够按图片找文字也能按文字找图片

总结：CLIP通过维护相似图像向量和文本向量在统一特征空间下指向方向一致性，达到多模态信息融合目的。

### 一些常见的 多模态（图文俩模态）损失函数设计

ITC : CLIP使用的方法

ITM ：把图文一起送到多模态transformer里做cross att之后，把吐出来的cls token做二分类判断图文是否匹配

MLM : 把文本随机mask掉，让模型通过图像信息对文本信息做完形填空

**idea!**
感觉除了MLM不太好实现，ITC和ITM的方法也能放在 RGB 和 Depth 模态的模型里 ， 无非是把文本信息换成Depth信息，然后做匹配而已。


### ALBEF

做的工作其实就是用CLIP把文本token和图像token先投射到同一个特征空间下进行对齐

使用self-distilation方法改善训练数据noisy问题

12层图像编码 ， 6层文本编码 ， 6层特征融合

特征融合部分用 ITM 和 MLM ， ITM 的 hard negative样本使用前面类CLIP部分得到的余弦相似度较高的样本进行训练

### VLMO 

使用类MoE架构，FFN 分三类权重 ，V、L、VL，分别用来专门处理图像、文本、特征融合，根据下游任务不同选择性激活

先用NLP和CV领域大型数据集pre-train V和L ，然后用多模态数据集训练VL

一个有意思的现象 ： 第一步现在vision上pre train ， 然后把 mha权重froze再pre train文本，效果不错。但是反过来就不太行，可能说明视觉这一块包含的信息熵还是很多，需要比较多的模型参数去处理视觉信息。

#### DAY1 总结

由于对多模态模型不太熟悉所以扫了一遍 text 和 vision的多模态模型

大部分方法都是全部基于transformer token体系下的架构，就是把text token 和 vision token放进去一块过mha

接下来看看CNN架构的多模态模型，CNN的多模态应该就是都是基于图像信息RGB Depth等模态做的工作了

---



